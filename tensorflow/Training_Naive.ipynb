{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba712a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two vocabulary dictionaries have been built!\n",
      "Please call \u001b[1mX.vocab_to_idx | X.idx_to_vocab\u001b[0m to find out more where [X] stands for the name you used for this TextVectorizer class.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from naive_models import LSTM, SimpleRNN\n",
    "import numpy as np\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "V = TextVectorizer(tokenizer=list)\n",
    "V.build_vocab(['a', 'b', 'c', 'd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243407b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, \n",
    "              embd_dim, \n",
    "              vocab_size, \n",
    "              num_class):\n",
    "\n",
    "    model_name = model_name.lower()\n",
    "    if model_name == 's-rnn':\n",
    "        model = SimpleRNN\n",
    "    elif model_name == 'lstm':\n",
    "        model = LSTM\n",
    "    else:\n",
    "        raise ValueError(f\"Only s-rnn and lstm models are allowed.\")\n",
    "    \n",
    "    model = model(vocab_size, num_class, embd_dim)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(5e-4, clipnorm=1.0), \n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=[[keras.metrics.BinaryAccuracy(), \n",
    "                            keras.metrics.Precision(), \n",
    "                            keras.metrics.Recall()]])\n",
    "    return model\n",
    "\n",
    "\n",
    "def do_train_and_evaluate(model_name, \n",
    "                          embd_dim, \n",
    "                          train_path,  \n",
    "                          epoch_num,\n",
    "                          num_class=2,\n",
    "                          batch_size=128,\n",
    "                          encoder = V,\n",
    "                          transform=transform,\n",
    "                          vocab_size=len(V),\n",
    "                          max_text_len=None,\n",
    "                          val_split=0.,\n",
    "                          earlystop=False, \n",
    "                          monitor='val_binary_accuracy', \n",
    "                          patience=10):\n",
    "    \n",
    "    if earlystop:\n",
    "        callbacks = [EarlyStopping(monitor=monitor, patience=patience)]\n",
    "        val_split = 0.2\n",
    "    else:\n",
    "        callbacks = None\n",
    "    \n",
    "    test1_path, test2_path = get_two_test_fpathes(train_path)\n",
    "    train, test1, test2 = load_dataset([train_path, \n",
    "                                        test1_path, \n",
    "                                        test2_path])\n",
    "    \n",
    "    train_X, train_Y = transform(train, encoder, num_class,\n",
    "                                 shuffle=True, max_len=max_text_len)\n",
    "    \n",
    "    test1_X, test1_Y = transform(test1, encoder, \n",
    "                                 num_class, max_text_len)\n",
    "    \n",
    "    test2_X, test2_Y = transform(test2, encoder, \n",
    "                                 num_class, max_text_len)\n",
    "    \n",
    "    model = get_model(model_name, embd_dim, vocab_size, num_class)\n",
    "    model.fit(train_X, train_Y, epochs=epoch_num, \n",
    "              validation_split=val_split,\n",
    "              batch_size=batch_size, callbacks=callbacks)\n",
    "        \n",
    "    test1_res = model.evaluate(test1_X, test1_Y, batch_size=1000, verbose=0)[1:]\n",
    "    test2_res = model.evaluate(test2_X, test2_Y, batch_size=1000, verbose=0)[1:]\n",
    "    \n",
    "    return test1_res, test2_res\n",
    "\n",
    "\n",
    "def get_results(train_path, model_name, embd_dim, r, \n",
    "                epoch_num, batch_size, \n",
    "                earlystop=False, patience=0):\n",
    "    \n",
    "    metadata = train_path.split('/')[2:-1] + [model_name, embd_dim, \n",
    "                                              earlystop, r]\n",
    "    \n",
    "    test1_res, test2_res = do_train_and_evaluate(model_name, \n",
    "                                                 embd_dim, \n",
    "                                                 train_path, \n",
    "                                                 epoch_num, \n",
    "                                                 patience=patience,\n",
    "                                                 earlystop=earlystop)\n",
    "    res1 = metadata + ['Test1'] + test1_res\n",
    "    res2 = metadata + ['Test2'] + test2_res\n",
    "    return [res1, res2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae0b6ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Round # 1 ====================\n",
      "\n",
      "SubLang: SP8; Size: 100k; Model: s-rnn; Embd Dim: 100; Earlystop: True\n",
      "\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 11s 16ms/step - loss: 0.1693 - binary_accuracy: 0.9352 - precision_431: 0.9352 - recall_431: 0.9352 - val_loss: 0.1145 - val_binary_accuracy: 0.9559 - val_precision_431: 0.9559 - val_recall_431: 0.9559\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.1005 - binary_accuracy: 0.9632 - precision_431: 0.9632 - recall_431: 0.9632 - val_loss: 0.0735 - val_binary_accuracy: 0.9718 - val_precision_431: 0.9718 - val_recall_431: 0.9718\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0693 - binary_accuracy: 0.9757 - precision_431: 0.9757 - recall_431: 0.9757 - val_loss: 0.0762 - val_binary_accuracy: 0.9750 - val_precision_431: 0.9750 - val_recall_431: 0.9750\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0583 - binary_accuracy: 0.9812 - precision_431: 0.9812 - recall_431: 0.9812 - val_loss: 0.0460 - val_binary_accuracy: 0.9861 - val_precision_431: 0.9861 - val_recall_431: 0.9861\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.0411 - binary_accuracy: 0.9873 - precision_431: 0.9873 - recall_431: 0.9873 - val_loss: 0.0323 - val_binary_accuracy: 0.9885 - val_precision_431: 0.9885 - val_recall_431: 0.9885\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.0296 - binary_accuracy: 0.9908 - precision_431: 0.9908 - recall_431: 0.9908 - val_loss: 0.0225 - val_binary_accuracy: 0.9927 - val_precision_431: 0.9927 - val_recall_431: 0.9927\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.0222 - binary_accuracy: 0.9935 - precision_431: 0.9935 - recall_431: 0.9935 - val_loss: 0.0285 - val_binary_accuracy: 0.9916 - val_precision_431: 0.9916 - val_recall_431: 0.9916\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.0174 - binary_accuracy: 0.9954 - precision_431: 0.9954 - recall_431: 0.9954 - val_loss: 0.0152 - val_binary_accuracy: 0.9962 - val_precision_431: 0.9962 - val_recall_431: 0.9962\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.0111 - binary_accuracy: 0.9969 - precision_431: 0.9969 - recall_431: 0.9969 - val_loss: 0.0089 - val_binary_accuracy: 0.9976 - val_precision_431: 0.9976 - val_recall_431: 0.9976\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.0084 - binary_accuracy: 0.9978 - precision_431: 0.9978 - recall_431: 0.9978 - val_loss: 0.0139 - val_binary_accuracy: 0.9961 - val_precision_431: 0.9961 - val_recall_431: 0.9961\n"
     ]
    }
   ],
   "source": [
    "filepathes = get_filepathes_from_dir('Experimental Data/Data', \n",
    "                                     include_sub_dir=True, \n",
    "                                     file_format='Training.txt')\n",
    "\n",
    "filepathes = sort_filepathes(filepathes)\n",
    "columns = ['Lang Class', 'Lang Subclass', 'Train Size', \n",
    "           'Model', 'Embd Dim', 'EarlyStop', 'Round #', \n",
    "           'Test Set', 'Accuracy', 'Recall', 'Precision']\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for fpath in filepathes:\n",
    "    sl, s = fpath.split('/')[3:-1]\n",
    "    size = int(s.replace('k', ''))\n",
    "    \n",
    "    if size == 100:\n",
    "        end = 2; epoch_num = 10; patience = 2; batch_size = 2048\n",
    "        \n",
    "    elif size == 10:\n",
    "        end = 3; epoch_num = 50; patience = 5; batch_size = 512\n",
    "        \n",
    "    else:\n",
    "        end = 4; epoch_num = 100; patience = 10; batch_size = 128\n",
    "    \n",
    "    for model in ['lstm', 's-rnn']:\n",
    "        for dim in [10, 30, 100]:\n",
    "            for r in range(1, end):\n",
    "                for earlystop in [False, True]:\n",
    "                    print(f\"{'=' * 20} Round # {r} {'=' * 20}\\n\")\n",
    "                    print(f\"SubLang: {sl}; Size: {s}; Model: {model}; Embd Dim: {dim}; Earlystop: {earlystop}\\n\")\n",
    "                    \n",
    "                    result = get_results(fpath, model, dim, r, \n",
    "                                         epoch_num, batch_size, \n",
    "                                         earlystop, patience)\n",
    "                    \n",
    "                    results.extend(result)\n",
    "                    clear_output(wait=True)\n",
    "                    \n",
    "        pd.DataFrame(results, columns=columns).to_csv('tf_results/results_naive.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
