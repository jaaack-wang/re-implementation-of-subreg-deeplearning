{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba712a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two vocabulary dictionaries have been built!\n",
      "Please call \u001b[1mX.vocab_to_idx | X.idx_to_vocab\u001b[0m to find out more where [X] stands for the name you used for this TextVectorizer class.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from models import LSTM, SimpleRNN\n",
    "import numpy as np\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "np.random.seed(123)\n",
    "python_random.seed(123)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "V = TextVectorizer(tokenizer=list)\n",
    "V.build_vocab(['a', 'b', 'c', 'd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243407b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, \n",
    "              embd_dim, \n",
    "              vocab_size, \n",
    "              num_class):\n",
    "\n",
    "    model_name = model_name.lower()\n",
    "    if model_name == 's-rnn':\n",
    "        model = SimpleRNN\n",
    "    elif model_name == 'lstm':\n",
    "        model = LSTM\n",
    "    else:\n",
    "        raise ValueError(f\"Only s-rnn and lstm models are allowed.\")\n",
    "    \n",
    "    model = model(vocab_size, num_class, embd_dim)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(5e-4, clipnorm=1.0), \n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=[[keras.metrics.BinaryAccuracy(), \n",
    "                            keras.metrics.Precision(), \n",
    "                            keras.metrics.Recall()]])\n",
    "    return model\n",
    "\n",
    "\n",
    "def do_train_and_evaluate(model_name, \n",
    "                          embd_dim, \n",
    "                          train_path,  \n",
    "                          epoch_num,\n",
    "                          num_class=2,\n",
    "                          batch_size=128,\n",
    "                          encoder = V,\n",
    "                          transform=transform,\n",
    "                          vocab_size=len(V),\n",
    "                          max_text_len=None,\n",
    "                          val_split=0.,\n",
    "                          earlystop=False, \n",
    "                          monitor='val_loss', \n",
    "                          patience=10):\n",
    "    \n",
    "    if earlystop:\n",
    "        callbacks = [EarlyStopping(monitor=monitor, patience=patience)]\n",
    "        val_split = 0.2\n",
    "    else:\n",
    "        callbacks = None\n",
    "    \n",
    "    test1_path, test2_path = get_two_test_fpathes(train_path)\n",
    "    train, test1, test2 = load_dataset([train_path, \n",
    "                                        test1_path, \n",
    "                                        test2_path])\n",
    "    \n",
    "    train_X, train_Y = transform(train, encoder, num_class,\n",
    "                                 shuffle=True, max_len=max_text_len)\n",
    "    \n",
    "    test1_X, test1_Y = transform(test1, encoder, \n",
    "                                 num_class, max_text_len)\n",
    "    \n",
    "    test2_X, test2_Y = transform(test2, encoder, \n",
    "                                 num_class, max_text_len)\n",
    "    \n",
    "    model = get_model(model_name, embd_dim, vocab_size, num_class)\n",
    "    model.fit(train_X, train_Y, epochs=epoch_num, \n",
    "              validation_split=val_split,\n",
    "              batch_size=batch_size, callbacks=callbacks)\n",
    "        \n",
    "    test1_res = model.evaluate(test1_X, test1_Y, batch_size=1000, verbose=0)[1:]\n",
    "    test2_res = model.evaluate(test2_X, test2_Y, batch_size=1000, verbose=0)[1:]\n",
    "    \n",
    "    return test1_res, test2_res\n",
    "\n",
    "\n",
    "def get_results(train_path, model_name, embd_dim, r, \n",
    "                epoch_num, batch_size, \n",
    "                earlystop=False, patience=0):\n",
    "    \n",
    "    metadata = train_path.split('/')[2:-1] + [model_name, embd_dim, \n",
    "                                              earlystop, r]\n",
    "    \n",
    "    test1_res, test2_res = do_train_and_evaluate(model_name, \n",
    "                                                 embd_dim, \n",
    "                                                 train_path, \n",
    "                                                 epoch_num, \n",
    "                                                 patience=patience,\n",
    "                                                 earlystop=earlystop)\n",
    "    res1 = metadata + ['Test1'] + test1_res\n",
    "    res2 = metadata + ['Test2'] + test2_res\n",
    "    return [res1, res2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae0b6ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Round # 1 ====================\n",
      "\n",
      "SubLang: SP8; Size: 100k; Model: s-rnn; Embd Dim: 100; Earlystop: True\n",
      "\n",
      "Epoch 1/20\n",
      "625/625 [==============================] - 12s 17ms/step - loss: 0.1393 - binary_accuracy: 0.9460 - precision_71: 0.9460 - recall_71: 0.9460 - val_loss: 0.0847 - val_binary_accuracy: 0.9711 - val_precision_71: 0.9711 - val_recall_71: 0.9711\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.0707 - binary_accuracy: 0.9754 - precision_71: 0.9754 - recall_71: 0.9754 - val_loss: 0.0653 - val_binary_accuracy: 0.9775 - val_precision_71: 0.9775 - val_recall_71: 0.9775\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.0467 - binary_accuracy: 0.9839 - precision_71: 0.9839 - recall_71: 0.9839 - val_loss: 0.0427 - val_binary_accuracy: 0.9862 - val_precision_71: 0.9862 - val_recall_71: 0.9862\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.0362 - binary_accuracy: 0.9878 - precision_71: 0.9878 - recall_71: 0.9878 - val_loss: 0.0298 - val_binary_accuracy: 0.9898 - val_precision_71: 0.9898 - val_recall_71: 0.9898\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.0285 - binary_accuracy: 0.9904 - precision_71: 0.9904 - recall_71: 0.9904 - val_loss: 0.0275 - val_binary_accuracy: 0.9915 - val_precision_71: 0.9915 - val_recall_71: 0.9915\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.0193 - binary_accuracy: 0.9936 - precision_71: 0.9936 - recall_71: 0.9936 - val_loss: 0.0413 - val_binary_accuracy: 0.9870 - val_precision_71: 0.9870 - val_recall_71: 0.9870\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.0166 - binary_accuracy: 0.9946 - precision_71: 0.9946 - recall_71: 0.9946 - val_loss: 0.0168 - val_binary_accuracy: 0.9942 - val_precision_71: 0.9942 - val_recall_71: 0.9942\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0133 - binary_accuracy: 0.9958 - precision_71: 0.9958 - recall_71: 0.9958 - val_loss: 0.0097 - val_binary_accuracy: 0.9971 - val_precision_71: 0.9971 - val_recall_71: 0.9971\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0098 - binary_accuracy: 0.9970 - precision_71: 0.9970 - recall_71: 0.9970 - val_loss: 0.0047 - val_binary_accuracy: 0.9987 - val_precision_71: 0.9987 - val_recall_71: 0.9987\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.0061 - binary_accuracy: 0.9983 - precision_71: 0.9983 - recall_71: 0.9983 - val_loss: 0.0051 - val_binary_accuracy: 0.9984 - val_precision_71: 0.9984 - val_recall_71: 0.9984\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.0071 - binary_accuracy: 0.9979 - precision_71: 0.9979 - recall_71: 0.9979 - val_loss: 0.0057 - val_binary_accuracy: 0.9980 - val_precision_71: 0.9980 - val_recall_71: 0.9980\n"
     ]
    }
   ],
   "source": [
    "filepathes = get_filepathes_from_dir('Experimental Data/Data', \n",
    "                                     include_sub_dir=True, \n",
    "                                     file_format='Training.txt')\n",
    "\n",
    "filepathes = sort_filepathes(filepathes)\n",
    "columns = ['Lang Class', 'Lang Subclass', 'Train Size', \n",
    "           'Model', 'Embd Dim', 'EarlyStop', 'Round #', \n",
    "           'Test Set', 'Accuracy', 'Recall', 'Precision']\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "for fpath in filepathes[-3:]:\n",
    "    sl, s = fpath.split('/')[3:-1]\n",
    "    size = int(s.replace('k', ''))\n",
    "    \n",
    "    if size == 100:\n",
    "        end = 2; epoch_num = 20; patience = 2; batch_size = 2048\n",
    "        \n",
    "    elif size == 10:\n",
    "        end = 3; epoch_num = 70; patience = 5; batch_size = 512\n",
    "        \n",
    "    else:\n",
    "        end = 4; epoch_num = 200; patience = 10; batch_size = 128\n",
    "    \n",
    "    for model in ['lstm', 's-rnn']:\n",
    "        for dim in [10, 30, 100]:\n",
    "            for r in range(1, end):\n",
    "                for earlystop in [False, True]:\n",
    "                    print(f\"{'=' * 20} Round # {r} {'=' * 20}\\n\")\n",
    "                    print(f\"SubLang: {sl}; Size: {s}; Model: {model}; Embd Dim: {dim}; Earlystop: {earlystop}\\n\")\n",
    "                    \n",
    "                    result = get_results(fpath, model, dim, r, \n",
    "                                         epoch_num, batch_size, \n",
    "                                         earlystop, patience)\n",
    "                    \n",
    "                    results.extend(result)\n",
    "                    clear_output(wait=True)\n",
    "\n",
    "        pd.DataFrame(results, columns=columns).to_csv('tf_results/SP8_results_with_added_fc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
